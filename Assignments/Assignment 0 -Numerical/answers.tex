\documentclass{article}
\usepackage{verbatim}
\usepackage{hyperref}
\usepackage{tcolorbox}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{threeparttable}
\usepackage{float}

\sisetup{
  detect-all,
  output-exponent-marker = \mathrm{e},
  table-number-alignment = center
}


\providecommand{\e}[1]{\ensuremath{\times 10^{#1}}}
\setlength{\parindent}{0pt}

\begin{document}

\title{Answers to Empirical IO I: Problem Set 0} 
\author{}
\date{}
\maketitle


\section*{Part 0: Logit Inclusive Value}

\begin{tcolorbox}

The logit inclusive value or $IV = \log \sum_{i=0}^N \exp[x_i]$.
\begin{enumerate}
\item Show that the this function is everywhere convex if $x_0=0$.
\item A common problem in practice is that if one of the $x_i > 600$ that we have an ``overflow'' error on a computer. In this case $\exp[600] \approx 10^{260}$ which is too large to store with any real precision, especially if another $x$ has a different scale (say $x_2=10$). A common ``trick'' is to subtract off $m_i = \max_i x_i$ from all $x_i$.  Show how to implement the trick and get the correct value of $IV$. If you get stuck take a look at Wikipedia.
\item Compare your function to $\mathtt{scipy.special.logsumexp}$. Does it appear to suffer from underflow/overflow? Does it use the $\max$ trick?
\end{enumerate}

    \end{tcolorbox}

    \vspace{5mm}

    \textbf{1.} We have the function:
    \[
    IV = f(x_1,\dots,x_N) \;=\; \log\!\Big(1+e^{x_1}+\cdots+e^{x_N}\Big),
    \]
    
    Fix $x=(x_1,\dots,x_N)$ and a direction $h=(h_1,\dots,h_N)$.  
    Consider the single-variable function:
    \[
    g(\theta) \;=\; f(x+\theta h)
    = \log\!\Big(1+\sum_{i=1}^N e^{\,x_i+\theta h_i}\Big).
    \]
    
    First derivative:
    \[
    g'(\theta)
    = \frac{\sum_{i=1}^N h_i e^{x_i+\theta h_i}}
           {1+\sum_{i=1}^N e^{x_i+\theta h_i}}.
    \]
    
    Second derivative. Writing $a_i(\theta)=e^{x_i+\theta h_i}$,
    \[
    g''(\theta)
    = \frac{\Big(1+\sum_{i=1}^N a_i(\theta)\Big)\Big(\sum_{i=1}^N h_i^2 a_i(\theta)\Big)
          -\Big(\sum_{i=1}^N h_i a_i(\theta)\Big)^2}
           {\Big(1+\sum_{i=1}^N a_i(\theta)\Big)^2}.
    \]
    
    Expanding the numerator gives
    \[
    \Big(1+\sum_{i=1}^N a_i(\theta)\Big)\Big(\sum_{i=1}^N h_i^2 a_i(\theta)\Big)
    - \Big(\sum_{i=1}^N h_i a_i(\theta)\Big)^2
    = \sum_{1 \le i < j \le N} a_i(\theta)a_j(\theta)\,(h_i-h_j)^2.
    \]
    
    Hence
    \[
    g''(\theta)
    = \frac{\sum_{1 \le i < j \le N}
    e^{x_i+\theta h_i}\, e^{x_j+\theta h_j}\,(h_i-h_j)^2}
    {\Big(1+\sum_{k=1}^N e^{x_k+\theta h_k}\Big)^2}
    \;\;\ge\; 0.
    \]
    
    Since $g''(\theta)\ge 0$ for all $\theta$, $g$ is convex in $\theta$.  
    Therefore $IV$ is everywhere convex.

    \vspace{5mm}

    \textbf{2.} See Julia code. Used test vector $x = (10,11,12,1000)$ and showed that naive implementation of inclusive value led to infinte result, whereas "trick" gave value $\approx 1000$

    \vspace{5mm}

    \textbf{3.} Compared to the Julia equivalent, StatsFuns. This produced the same result and does not appear to suffer from overflow. Underflow should be harmless here: small terms that underflow to 0 won't contribute anything to the IV anyway.






\section*{Part 1: Markov Chains}

\begin{tcolorbox}
Consider the following Markov TPM:\\
Let $P = \{p_{i,j} \}$ be an $n \times n$ transition matrix of a Markov process where $\{p_{i,j} \}$ is interpreted as the probability that the system, when it is in state $i$, will move to state $j$. If we denote by $\pi_t [ \pi_{t,1} , \pi_{t,2}, \ldots, \pi_{t,n}]$ the probability mass function of the system over the $n$ states then $\pi_{t,j}$ evolves according to 
\begin{eqnarray*}
\pi_{t+1,j} = \sum_{i=1}^n p_{i,j} \pi_{t,i}
\end{eqnarray*}
Then we can write the state to state transition matrix as :
\begin{eqnarray*}
\pi_{t+1}& &= \pi_t P \\
P &=&
\left[ {\begin{array}{ccc}
    0.2&    0.4&    0.4\\
    0.1&    0.3&    0.6\\
    0.5&    0.1&    0.4\\
 \end{array} } \right]
\end{eqnarray*}
We're interested in the ergodic distribution $\pi P = \pi$. This is similar to the transition matrix infinitely many periods into the future $P^{\infty}$. Write a function that computes the ergodic distribution of the matrix $P$ by examining the properly rescaled eigenvectors and compare your result to $P^{100}$. Here I recommend $\mathtt{numpy.linalg.matrix\_power}$ and $\mathtt{numpy.linalg.eig}$. (A common mistake is element-wise exponentiation of the matrix).
\end{tcolorbox}

See Julia code. Both approaches lead to the same values of $\pi$ to 6 places:
$(0.310345, 0.241379, 0.448276)$

\section*{Part 2: Numerical Integration}

\begin{tcolorbox}
\textit{Note: The \texttt{scipy} quadrature routines may return a set of nodes/weights that correspond to integrating $e^{-x^2}$ over $(\infty,\infty)$, while the nodes/weights available at \url{http://www.sparse-grids.de} may not. It is always important to make sure you understand what your nodes/weights correspond to. One way to do this is to integrate some simple functions $f(x)=1,f(x)=x,f(x)=x^2$ where you know the analytic result and see what the quadrature routine gives as the answer.}\\

In this part we will look to calculation the logit choice probability $p(X,\theta)$ by numerical integration:\\
 $p(X,\theta) =\int_{-\infty}^{\infty} \frac{\exp(\beta_i X)}{1+ \exp(\beta_i X)} f(\beta_i | \theta) \partial \beta_i$. \\
 Assume $f \sim N(0.5,2)$ and that $X = 0.5$.
\end{tcolorbox}

 
\begin{enumerate}

\begin{tcolorbox}
\item Create the function in an Python called $\mathtt{binomiallogit}$. (It should take $\beta$ the item you integrate over as its argument, it should take the PDF $\mathtt{scipy.stats.norm.pdf}$ as an optional argument).

\item Integrate the function using Python's $\mathtt{scipy.integrate.quad}$ command and setting the tolerance to $1\e{-14}$.  Treat this a the ``true'' value.
\begin{comment}
\begin{verbatim}
[Ftrue,nevals]=quad(@binomiallogit,-10,10,1e-14);
z=randn(100,1);
Fmc=sum(binomiallogitnopdf(z))./length(z);
Fgh=binomiallogitnopdf(x)'*(w./sum(w));
\end{verbatim}
\end{comment}

\item Integrate the function by taking 20 and 400 Monte Carlo draws from $f$ and computing the sample mean.
\item Integrate the function using Gauss-Hermite quadrature for $k=4, 12$ (Try some odd ones too). Obtain the quadrature points and nodes from the internet. Gauss-Hermite quadrature assumes a weighting function of $\exp[-x^2]$, you will need a change of variables to integrate over a normal density.[See my notes] You also need to pay attention to the constant of integration.
\end{tcolorbox}

\textbf{1.} See Julia code.

\textbf{2.} "True" value $\approx0.551$

\textbf{3.} See Julia code.

\textbf{4.} To apply Gauss--Hermite quadrature, recall that the standard formula approximates
\[
\int_{-\infty}^\infty e^{-x^2}\, g(x)\, dx \;\approx\; \sum_{i=1}^k w_i g(x_i),
\]
where $\{x_i,w_i\}$ are the Hermite nodes and weights. Our target integral is an
expectation with respect to a normal density:
\[
\int_{-\infty}^\infty \sigma(\beta X)\, f(\beta)\, d\beta, 
\quad f(\beta)=\frac{1}{\sqrt{2\pi}\,\sigma}\,
\exp\!\Big(-\tfrac{(\beta-\mu)^2}{2\sigma^2}\Big).
\]

First set $z = (\beta-\mu)/\sigma$, so that $\beta = \mu + \sigma z$ and $z \sim N(0,1)$.
This gives
\[
\int_{-\infty}^\infty \sigma\!\big((\mu+\sigma z)X\big)\,
\phi(z)\,dz, 
\quad \phi(z) = \tfrac{1}{\sqrt{2\pi}} e^{-z^2/2}.
\]

Next substitute $z = \sqrt{2}\,x$, yielding
\[
\int_{-\infty}^\infty \sigma\!\big((\mu+\sigma\sqrt{2}x)X\big)\,
\frac{1}{\sqrt{\pi}} e^{-x^2}\, dx.
\]

This now matches the Gauss--Hermite weight $e^{-x^2}$. Therefore,
\[
\int \sigma(\beta X) f(\beta)\,d\beta 
\;\approx\; \frac{1}{\sqrt{\pi}}
\sum_{i=1}^k w_i \,\sigma\!\big((\mu+\sigma\sqrt{2}\,x_i)X\big).
\]

Hence the Hermite nodes $x_i$ are rescaled to 
$\beta_i = \mu+\sigma\sqrt{2}\,x_i$,
and the effective weights are $w_i/\sqrt{\pi}$, which sum to one. This ensures
the quadrature formula is consistent with integration under the normal density.

See Julia code for implementation.


\begin{tcolorbox}
\item Compare results to the Monte Carlo results. \textit{Make sure your quadrature weights sum to 1!}
\begin{comment}
\begin{table}[htdp]
\caption{True value: 0.5515}
\begin{center}
\begin{tabular}{l r r r }
Method & Points & Error\\
quad & 2597 & 1e-14 \\
monte carlo & 100 & 0.0166\\
Gauss Hermite & 4 & 0.0044234\\
Gauss Hermite & 12 & 0.0044469\\
\end{tabular}
\end{center}
\end{table}
\end{comment}

\item Repeat the exercise in two dimensions where $\mu = (0.5,1), \sigma = (2,1)$, and $X=(0.5,1)$.
\item Put everything into two tables (one for the 1-D integral, one for the 2-D integral). Showing the error from the ``true'' value and the number of points used in the evaluation.
\begin{comment}
\begin{table}[htdp]
\caption{2-D Results True value: 0.7145}
\begin{center}
\begin{tabular}{l r r r }
Method & Points & Error\\
quad & n/a & 1e-14 \\
monte carlo & 100 &  0.0174\\
Gauss Hermite(PR) & 25 & 0.0250\\
SGI-GQN & 13 & 0.0091\\
SGI-KPN & 17 & -6.9355e-04\\
\end{tabular}
\end{center}
\end{table}%
\end{comment}
\item Now Construct a new function $\mathtt{binomiallogitmixture}$ that takes a vector for $X$ and returns a vector of binomial probabilities (appropriately integrated over $f(\beta_i | \theta)$ for the 1-D mixture). It should be obvious that Gauss-Hermite is the most efficient way to do this. \textit{Do NOT use loops}
\end{tcolorbox}
\end{enumerate}

\textbf{5.} All values are reasonably close to the "true" value (i.e. 2 places) but the GH values are even closer (3 places +).

\textbf{6.} See Julia code.

\begin{table}[H]
\textbf{7. }
\begin{threeparttable}
\caption{Numerical Integration Results}
\begin{tabular}{@{}p{0.48\linewidth} p{0.48\linewidth}@{}}
{\centering \textbf{1-D Results\\(true value: 0.551493)}\\[0.5ex]
\begingroup\setlength{\tabcolsep}{6pt}
\begin{tabular}{l S[table-format=4.0] S}
\toprule
Method & {Points} & {Error}\\
\midrule
Monte Carlo & 20 & \num{8.756e-03} \\
Monte Carlo & 400 & \num{5.735e-03} \\
Gauss--Hermite & 4 & \num{1.794e-04} \\
Gauss--Hermite & 9 & \num{9.517e-07} \\
Gauss--Hermite & 12 & \num{6.857e-08} \\
\bottomrule
\end{tabular}
\endgroup
} &
{\centering \textbf{2-D Results\\(true value: 0.714484)}\\[0.5ex]
\begingroup\setlength{\tabcolsep}{6pt}
\begin{tabular}{l S[table-format=4.0] S}
\toprule
Method & {Points} & {Error}\\
\midrule
Monte Carlo & 20 & \num{1.418e-02} \\
Monte Carlo & 400 & \num{4.135e-04} \\
Gauss--Hermite (4×4) & 16 & \num{1.124e-04} \\
Gauss--Hermite (9×9) & 81 & \num{4.193e-08} \\
Gauss--Hermite (12×12) & 144 & \num{4.464e-09} \\
\bottomrule
\end{tabular}
\endgroup
} \\
\end{tabular}
\begin{tablenotes}
\item \footnotesize Note: For Monte Carlo, ``Points'' = random draws; for Gauss--Hermite, ``Points'' = quadrature nodes.
\end{tablenotes}
\end{threeparttable}
\end{table}

\textbf{8.} See Julia code.


%
%\section*{Part 3: Functional Approximation}
%What is interpolation? Suppose you have a collection of $n$ points in $R^2$ and $D=\{(x_1,y_1),\ldots,(x_n,y_n)\}$  where $y_i = g(x_i)$, $x_i \neq x_j$  $\forall i \neq j$ and you want to construct a function that closely fits these data points. Interpolation offers a variety of different techniques that perform this task in a more or less smooth manner.\\
%
%
%We are interested in interpolating your $\mathtt{binomiallogitmixture}(x)$ from the previous part at 20 evenly spaced data points $(\mathtt{linspace})$ from $[-4,4]$. (You may want to work with an easier function such as $\mathtt{sin(x)}$ first.\\
%
%We are going to use Chebyshev regression to construct a degree $n$ polynomial that approximates a function $f$ for $x \in [a,b]$ using $m > n $ points. The implementation requires the following steps, which you should include in your function $\mathtt{chebfit(fname,a,b,n)}$.
%\begin{enumerate}
%\item Compute the $m$ Chebyshev interpolation nodes on $[-1,1]$, $z_k = - \cos\left(\frac{2k-1}{2m} \pi \right)$ for $k=1,\ldots, m$.
%\item Adjust the nodes to the $[a,b]$ interval $x_k = (1+z_k) \left(\frac{b-a}{2} \right) + a$, for $k=1,\ldots,m$.
%\item Evaluate the function $f$ at the approximation nodes $y_k = f(x_k)$  for $k=1,\ldots,m$
%\item Compute the Chebyshev polynomial coefficients via $a_i = \frac{\sum_{k=1}^m y_k T_i(z_k)}{\sum_{k=1}^m  T_i(z_k)^2}$ where the polynomials\\ $T_0(x) = 1, T_1(x) = x, T_{i+1}(x) = 2xT_i(x) - T_{i-1}(x)$. 
%\item Your function should return the nodes $z_k$ and the coefficients $c$.
%\item Write a new function that takes the coefficients $c$ and computes a (vectorized) approximation to your function
%\begin{eqnarray*}
%\hat{f}(x) = \sum_{i=0}^n a_i T_i \left( 2 \frac{x-a}{b-a} -1 \right)
%\end{eqnarray*}
%\item Compare your approximation to Python's built-in chebyshev polynomial fitting $\mathtt{numpy.polynomial.chebyshev.chebfit}$ as well as the built in cubic-spline interpolator $\mathtt{scipy.interpolate)}$ and see how it does. (Plot both approximations on $0.01$ spaced grid-points against the actual nodes).
%\item What happens to the approximations outside the domain? What happens as we vary $n$?
%
%\end{enumerate}

\end{document}

